{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "###要把另一个jupyter notebook关掉，不然这个训练时有影响，导致一训练服务就要重启"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 03:09:29.418821 140497473152768 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 6s 112us/sample - loss: 0.3420 - sparse_categorical_accuracy: 0.9017 - val_loss: 0.1789 - val_sparse_categorical_accuracy: 0.9483\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 5s 100us/sample - loss: 0.1598 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9611\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 5s 96us/sample - loss: 0.1163 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.1105 - val_sparse_categorical_accuracy: 0.9686\n",
      "history:\n",
      "{'sparse_categorical_accuracy': [0.90172, 0.95268, 0.9644], 'val_loss': [0.17890676038265227, 0.13857471915781497, 0.1104788906276226], 'loss': [0.3419858175897598, 0.15977194753885268, 0.11627530758261681], 'val_sparse_categorical_accuracy': [0.9483, 0.9611, 0.9686]}\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.1082 - sparse_categorical_accuracy: 0.9675\n",
      "evaluate:\n",
      "[0.1082186477124691, 0.9675]\n",
      "predict:\n",
      "[[9.6373697e-08 8.2183647e-08 1.2031219e-04 7.9352531e-04 1.4178224e-10\n",
      "  6.8006989e-07 4.3007850e-12 9.9908352e-01 2.7640309e-07 1.5287484e-06]\n",
      " [7.2129455e-07 1.2672909e-04 9.9928159e-01 5.7528092e-04 8.3215275e-14\n",
      "  1.6059768e-06 1.6569119e-06 1.5321676e-08 1.2436299e-05 8.9141517e-14]]\n"
     ]
    }
   ],
   "source": [
    "#1.一般的模型构造、训练、测试流程\n",
    "\n",
    "# 模型构造\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# 载入数据\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=3,\n",
    "         validation_data=(x_val, y_val))\n",
    "print('history:')\n",
    "print(history.history)\n",
    "\n",
    "result = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('evaluate:')\n",
    "print(result)\n",
    "pred = model.predict(x_test[:2])\n",
    "print('predict:')\n",
    "print(pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 5s 92us/sample - loss: 0.0942 - binary_true_postives: 7888.0000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 4s 82us/sample - loss: 0.0781 - binary_true_postives: 8305.0000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 4s 83us/sample - loss: 0.0670 - binary_true_postives: 8381.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc74c0fd160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.自定义损失和指标\n",
    "# 自定义指标只需继承Metric类， 并重写一下函数\n",
    "\n",
    "# _init_(self)，初始化。\n",
    "\n",
    "# update_state(self，y_true，y_pred，sample_weight = None)，它使用目标y_true和模型预测y_pred来更新状态变量。\n",
    "\n",
    "# result(self)，它使用状态变量来计算最终结果。\n",
    "\n",
    "# reset_states(self)，重新初始化度量的状态。\n",
    "# 这是一个简单的示例，显示如何实现CatgoricalTruePositives指标，该指标计算正确分类为属于给定类的样本数量\n",
    "\n",
    "class CatgoricalTruePostives(keras.metrics.Metric):\n",
    "    def __init__(self, name='binary_true_postives', **kwargs):\n",
    "        super(CatgoricalTruePostives, self).__init__(name=name, **kwargs)\n",
    "        self.true_postives = self.add_weight(name='tp', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred)\n",
    "        y_true = tf.equal(tf.cast(y_pred, tf.int32), tf.cast(y_true, tf.int32))\n",
    "        \n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            y_true = tf.multiply(sample_weight, y_true)\n",
    "            \n",
    "        return self.true_postives.assign_add(tf.reduce_sum(y_true))\n",
    "    \n",
    "    def result(self):\n",
    "        return tf.identity(self.true_postives)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.true_postives.assign(0.)\n",
    "        \n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[CatgoricalTruePostives()])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size=64, epochs=3)\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 9s 180us/sample - loss: 2.3582 - sparse_categorical_accuracy: 0.1129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc70c7ea390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以定义网络层的方式添加网络loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h1 = ActivityRegularizationLayer()(h1)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 10s 192us/sample - loss: 0.2991 - sparse_categorical_accuracy: 0.9131 - std_of_activation: 1.0222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc70c365cf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以以定义网络层的方式添加要统计的metric\n",
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h1 = MetricLoggingLayer()(h1)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 10s 191us/sample - loss: 2.3614 - sparse_categorical_accuracy: 0.1134 - std_of_activation: 0.3083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc70c0f32b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接在model上面加\n",
    "# 也可以以定义网络层的方式添加要统计的metric\n",
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h2 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h2)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.add_metric(keras.backend.std(inputs),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "model.add_loss(tf.reduce_sum(h1)*0.1)\n",
    "\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "40000/40000 [==============================] - 9s 216us/sample - loss: 2.3014 - sparse_categorical_accuracy: 0.1141 - std_of_activation: 0.3087 - val_loss: 2.3014 - val_sparse_categorical_accuracy: 0.1115 - val_std_of_activation: 0.3066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc814566e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理使用validation_data传入测试数据，还可以使用validation_split划分验证数据\n",
    "\n",
    "# ps:validation_split只能在用numpy数据训练的情况下使用\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.7975 - sparse_categorical_accuracy: 0.7902 - val_loss: 0.4150 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.3780 - sparse_categorical_accuracy: 0.8886 - val_loss: 0.3006 - val_sparse_categorical_accuracy: 0.8854\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.3320 - sparse_categorical_accuracy: 0.9023 - val_loss: 0.2650 - val_sparse_categorical_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6e0f4eb00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.使用tf.data构造数据\n",
    "def get_compiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "    h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "    h2 = layers.Dense(64, activation='relu')(h1)\n",
    "    outputs = layers.Dense(10, activation='softmax')(h2)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                 loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "model = get_compiled_model()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "# model.fit(train_dataset, epochs=3)\n",
    "# steps_per_epoch 每个epoch只训练几步\n",
    "# validation_steps 每次验证，验证几步\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=100,\n",
    "         validation_data=val_dataset, validation_steps=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 2.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n",
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 5s 101us/sample - loss: 0.3616 - sparse_categorical_accuracy: 0.9034\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 5s 94us/sample - loss: 0.1748 - sparse_categorical_accuracy: 0.9510\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 5s 93us/sample - loss: 0.1295 - sparse_categorical_accuracy: 0.9636\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 5s 95us/sample - loss: 0.1036 - sparse_categorical_accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6d144a908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.样本权重和类权重\n",
    "# “样本权重”数组是一个数字数组，用于指定批处理中每个样本在计算总损失时应具有多少权重。 它通常用于不平衡的分类问题（这个想法是为了给予很少见的类更多的权重）。 当使用的权重是1和0时，该数组可以用作损失函数的掩码（完全丢弃某些样本对总损失的贡献）。\n",
    "\n",
    "# “类权重”dict是同一概念的更具体的实例：它将类索引映射到应该用于属于该类的样本的样本权重。 例如，如果类“0”比数据中的类“1”少两倍，则可以使用class_weight = {0：1.，1：0.5}。\n",
    "# --------------------- \n",
    "# 增加第5类的权重\n",
    "import numpy as np\n",
    "#类权重\n",
    "model = get_compiled_model()\n",
    "class_weight = {i:1.0 for i in range(10)}#共0-9 10个类别，每个类别权重是1\n",
    "class_weight[5] = 2.0#第5类权重是2\n",
    "print(class_weight)\n",
    "model.fit(x_train, y_train,\n",
    "         class_weight=class_weight,\n",
    "         batch_size=64,\n",
    "         epochs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 5s 97us/sample - loss: 0.3736 - sparse_categorical_accuracy: 0.9008\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 5s 92us/sample - loss: 0.1703 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 5s 92us/sample - loss: 0.1245 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 5s 92us/sample - loss: 0.1004 - sparse_categorical_accuracy: 0.9716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6d0f04c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本权重\n",
    "#y_train是0-9共10个类别，array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)\n",
    "model = get_compiled_model()\n",
    "sample_weight = np.ones(shape=(len(y_train),))##先将所有的训练样本权重都设为1\n",
    "sample_weight[y_train == 5] = 2.0##针对类别为5的样本，权重设为2\n",
    "model.fit(x_train, y_train,\n",
    "         sample_weight=sample_weight,\n",
    "         batch_size=64,\n",
    "         epochs=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# 5.多输入多输出模型\n",
    "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name='score_output')(x)\n",
    "class_output = layers.Dense(5, activation='softmax', name='class_output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])\n",
    "keras.utils.plot_model(model, 'multi_input_output_model.png'\n",
    "                       , show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 03:44:17.880979 140497473152768 training_utils.py:1237] Output score_output missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to score_output.\n"
     ]
    }
   ],
   "source": [
    "# 可以为模型指定不同的loss和metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# 还可以指定loss的权重\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weight={'score_output': 2., 'class_output': 1.})\n",
    "\n",
    "# 可以把不需要传播的loss置0\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output': keras.losses.CategoricalCrossentropy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.使用回调\n",
    "# Keras中的回调是在训练期间（在epoch开始时，batch结束时，epoch结束时等）在不同点调用的对象，可用于实现以下行为：\n",
    "\n",
    "# 在培训期间的不同时间点进行验证（超出内置的每个时期验证）\n",
    "# 定期检查模型或超过某个精度阈值\n",
    "# 在训练似乎平稳时改变模型的学习率\n",
    "# 在训练似乎平稳时对顶层进行微调\n",
    "# 在培训结束或超出某个性能阈值时发送电子邮件或即时消息通知等等。\n",
    "# 可使用的内置回调有\n",
    "\n",
    "# ModelCheckpoint：定期保存模型。\n",
    "# EarlyStopping：当训练不再改进验证指标时停止培训。\n",
    "# TensorBoard：定期编写可在TensorBoard中显示的模型日志（更多细节见“可视化”）。\n",
    "# CSVLogger：将丢失和指标数据流式传输到CSV文件。\n",
    "# 等等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.3640 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.2200 - val_sparse_categorical_accuracy: 0.9337\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 98us/sample - loss: 0.1701 - sparse_categorical_accuracy: 0.9491 - val_loss: 0.1711 - val_sparse_categorical_accuracy: 0.9492\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 96us/sample - loss: 0.1248 - sparse_categorical_accuracy: 0.9625 - val_loss: 0.1607 - val_sparse_categorical_accuracy: 0.9537\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 98us/sample - loss: 0.0999 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1396 - val_sparse_categorical_accuracy: 0.9579\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 97us/sample - loss: 0.0836 - sparse_categorical_accuracy: 0.9752 - val_loss: 0.1341 - val_sparse_categorical_accuracy: 0.9595\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 97us/sample - loss: 0.0699 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.1487 - val_sparse_categorical_accuracy: 0.9585\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6d06f0e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1回调使用\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.3714 - sparse_categorical_accuracy: 0.8935\n",
      "Epoch 00001: val_loss improved from inf to 0.23461, saving model to mymodel_1.h5\n",
      "40000/40000 [==============================] - 4s 108us/sample - loss: 0.3688 - sparse_categorical_accuracy: 0.8942 - val_loss: 0.2346 - val_sparse_categorical_accuracy: 0.9307\n",
      "Epoch 2/3\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.1733 - sparse_categorical_accuracy: 0.9490\n",
      "Epoch 00002: val_loss improved from 0.23461 to 0.17526, saving model to mymodel_2.h5\n",
      "40000/40000 [==============================] - 4s 97us/sample - loss: 0.1730 - sparse_categorical_accuracy: 0.9491 - val_loss: 0.1753 - val_sparse_categorical_accuracy: 0.9473\n",
      "Epoch 3/3\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.1254 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 00003: val_loss did not improve from 0.17526\n",
      "40000/40000 [==============================] - 4s 98us/sample - loss: 0.1255 - sparse_categorical_accuracy: 0.9621 - val_loss: 0.1755 - val_sparse_categorical_accuracy: 0.9454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6d01daf98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint模型回调\n",
    "model = get_compiled_model()\n",
    "check_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='mymodel_{epoch}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=3,\n",
    "         batch_size=64,\n",
    "         callbacks=[check_callback],\n",
    "         validation_split=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态调整学习率\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 05:36:08.638334 140497473152768 callbacks.py:241] Method (on_train_batch_end) is slow compared to the batch update (0.121348). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 5s 115us/sample - loss: 0.0987 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.1501 - val_sparse_categorical_accuracy: 0.9537\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.0813 - sparse_categorical_accuracy: 0.9755 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9584\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 5s 113us/sample - loss: 0.0694 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.1306 - val_sparse_categorical_accuracy: 0.9613\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 4s 107us/sample - loss: 0.0594 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.1310 - val_sparse_categorical_accuracy: 0.9643\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 4s 107us/sample - loss: 0.0506 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.1411 - val_sparse_categorical_accuracy: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6c2837f28>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用tensorboard\n",
    "tensorboard_cbk = keras.callbacks.TensorBoard(log_dir='./full_path_to_your_logs')\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=5,\n",
    "         batch_size=64,\n",
    "         callbacks=[tensorboard_cbk],\n",
    "         validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.3580 - sparse_categorical_accuracy: 0.8991\n",
      "loss: 0.35710057483315466\n",
      "40000/40000 [==============================] - 5s 128us/sample - loss: 0.3571 - sparse_categorical_accuracy: 0.8994 - val_loss: 0.2361 - val_sparse_categorical_accuracy: 0.9268\n",
      "Epoch 2/3\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.1675 - sparse_categorical_accuracy: 0.9503\n",
      "loss: 0.16733438974022866\n",
      "40000/40000 [==============================] - 5s 120us/sample - loss: 0.1673 - sparse_categorical_accuracy: 0.9503 - val_loss: 0.1953 - val_sparse_categorical_accuracy: 0.9390\n",
      "Epoch 3/3\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.1215 - sparse_categorical_accuracy: 0.9626\n",
      "loss: 0.12134051643908024\n",
      "40000/40000 [==============================] - 5s 117us/sample - loss: 0.1213 - sparse_categorical_accuracy: 0.9627 - val_loss: 0.1490 - val_sparse_categorical_accuracy: 0.9566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6c2705898>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.2创建自己的回调方法\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "    def on_epoch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        print('\\nloss:',self.losses[-1])\n",
    "        \n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    LossHistory()\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Training loss (for one batch) at step 0: 2.3425493240356445\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.2850112915039062\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.1744418144226074\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.152705430984497\n",
      "Seen so far: 38464 samples\n",
      "epoch:  1\n",
      "Training loss (for one batch) at step 0: 2.0394225120544434\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.047250270843506\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.8622875213623047\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.8848987817764282\n",
      "Seen so far: 38464 samples\n",
      "epoch:  2\n",
      "Training loss (for one batch) at step 0: 1.7007428407669067\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.7132885456085205\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.4844062328338623\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.5439560413360596\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "#7.自己构造训练和验证循环\n",
    "# Get the model.\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 自己构造循环\n",
    "for epoch in range(3):\n",
    "    print('epoch: ', epoch)\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 开一个gradient tape, 计算梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            \n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.401185989379883\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.3178329467773438\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.1804118156433105\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.098374843597412\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.1739400029182434\n",
      "Validation acc: 0.35350000858306885\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.104534864425659\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.948502540588379\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.8732008934020996\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.7152416706085205\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.46643999218940735\n",
      "Validation acc: 0.5687999725341797\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.7561230659484863\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.5390644073486328\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.5327123403549194\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.3336381912231445\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.6126599907875061\n",
      "Validation acc: 0.6868000030517578\n"
     ]
    }
   ],
   "source": [
    "# 训练并验证\n",
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy() \n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "  \n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      \n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=834812, shape=(), dtype=float32, numpy=7.2620053>]\n",
      "[<tf.Tensor: id=834873, shape=(), dtype=float32, numpy=7.0907006>]\n"
     ]
    }
   ],
   "source": [
    "##　添加自己构造的loss, 每次只能看到最新一次训练增加的loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "    return inputs\n",
    "  \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "logits = model(x_train[:64])\n",
    "print(model.losses)\n",
    "logits = model(x_train[:64])\n",
    "logits = model(x_train[64: 128])\n",
    "logits = model(x_train[128: 192])\n",
    "print(model.losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 9.64475154876709\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.512377977371216\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.4083564281463623\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3370771408081055\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.330214500427246\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.3290836811065674\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.3323705196380615\n",
      "Seen so far: 25664 samples\n"
     ]
    }
   ],
   "source": [
    "# 将loss添加进求导中\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "      # Add extra losses created during this forward pass:\n",
    "      loss_value += sum(model.losses)\n",
    "      \n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
