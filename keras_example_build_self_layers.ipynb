{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.构建一个简单的网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# 1.自定义网络层就是定义该层的权重和输入到输出的计算过程（前向传播）\n",
    "#   （1）定义权重\n",
    "#         当知道输入的维度时，网络层权重可以在初始化时定义，权重可以用add_weight方法，也可以自己定义变量\n",
    "#         当不知道输入的维度时，需要重写build函数，定义权重，权重可以用add_weight方法，也可以自己定义变量\n",
    "#   （2）定义前向传播\n",
    "#        重写call方法\n",
    "# #2.使用子层递归构建网络层：先定义一个层，然后再定义层时调用之前定义好的层\n",
    "#     可通过该方法定义网络层，或者进行loss收集\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1500)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.14850608  0.02952327  0.01521497  0.24282955]\n",
      " [-0.14850608  0.02952327  0.01521497  0.24282955]\n",
      " [-0.14850608  0.02952327  0.01521497  0.24282955]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定义网络层就是：设置网络权重和输出到输入的计算过程\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        \n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.weight = tf.Variable(initial_value=w_init(\n",
    "            shape=(input_dim, unit), dtype=tf.float32), trainable=True)\n",
    "        \n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.bias = tf.Variable(initial_value=b_init(\n",
    "            shape=(unit,), dtype=tf.float32), trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "        \n",
    "x = tf.ones((3,5))\n",
    "my_layer = MyLayer(5, 4)\n",
    "out = my_layer(x)\n",
    "print(out)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.14427458  0.01830874  0.07907504  0.01994785]\n",
      " [-0.14427458  0.01830874  0.07907504  0.01994785]\n",
      " [-0.14427458  0.01830874  0.07907504  0.01994785]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#按上面构建网络层，图层会自动跟踪权重w和b，当然我们也可以直接用add_weight的方法构建权重\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.weight = self.add_weight(shape=(input_dim, unit),\n",
    "                                     initializer=keras.initializers.RandomNormal(),\n",
    "                                     trainable=True)\n",
    "        self.bias = self.add_weight(shape=(unit,),\n",
    "                                   initializer=keras.initializers.Zeros(),\n",
    "                                   trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "        \n",
    "x = tf.ones((3,5))\n",
    "my_layer = MyLayer(5, 4)\n",
    "out = my_layer(x)\n",
    "print(out)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3.]\n",
      "[6. 6. 6.]\n",
      "weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
      "non-trainable weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
      "trainable weight: []\n"
     ]
    }
   ],
   "source": [
    "#也可以设置不可训练的权重\n",
    "class AddLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32):\n",
    "        super(AddLayer, self).__init__()\n",
    "        self.sum = self.add_weight(shape=(input_dim,),\n",
    "                                     initializer=keras.initializers.Zeros(),\n",
    "                                     trainable=False)\n",
    "       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.sum.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.sum\n",
    "        \n",
    "x = tf.ones((3,3))\n",
    "my_layer = AddLayer(3)\n",
    "out = my_layer(x)\n",
    "print(out.numpy())\n",
    "out = my_layer(x)\n",
    "print(out.numpy())\n",
    "print('weight:', my_layer.weights)\n",
    "print('non-trainable weight:', my_layer.non_trainable_weights)\n",
    "print('trainable weight:', my_layer.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"{} build() is called.\".format(self.name))\n",
    "        self.kernel = self.add_variable(\"kernel\",\n",
    "        shape=[int(input_shape[-1]),\n",
    "        self.num_outputs])\n",
    "#         self.built = True\n",
    "\n",
    "    def call(self, input):\n",
    "        print(\"{} call() is called.\".format(self.name))\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dense_layer build() is called.\n",
      "my_dense_layer call() is called.\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(layer(tf.zeros([10, 5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dense_layer call() is called.\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "设置self.built = True\n",
    "此时build函数只会被调用一次，即在首次调用层时，第二次调用层时由于built=True，所以不会调用build函数\n",
    "\n",
    "不过好像不设置也不影响\n",
    "'''\n",
    "print(layer(tf.zeros([10, 5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02168566  0.17378914 -0.00253749]\n",
      " [ 0.02168566  0.17378914 -0.00253749]\n",
      " [ 0.02168566  0.17378914 -0.00253749]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.01057525 -0.04035868  0.05209654]\n",
      " [ 0.01057525 -0.04035868  0.05209654]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#当定义网络时不知道网络的维度是可以重写build()函数，用获得的shape构建网络\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.unit = unit\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(shape=(input_shape[-1], self.unit),\n",
    "                                     initializer=keras.initializers.RandomNormal(),\n",
    "                                     trainable=True)\n",
    "        self.bias = self.add_weight(shape=(self.unit,),\n",
    "                                   initializer=keras.initializers.Zeros(),\n",
    "                                   trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "        \n",
    "\n",
    "my_layer = MyLayer(3)\n",
    "x = tf.ones((3,5))\n",
    "out = my_layer(x)\n",
    "print(out)\n",
    "my_layer = MyLayer(3)\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "out = my_layer(x)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "#2.使用子层递归构建网络层\n",
    "class MyBlock(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyBlock, self).__init__()\n",
    "        self.layer1 = MyLayer(32)##mylayer的定义方式和myblock类似，是一个类，里面定义了自己的层，初始化之后返回输入和前向传播的结果\n",
    "        self.layer2 = MyLayer(16)\n",
    "        self.layer3 = MyLayer(2)\n",
    "    def call(self, inputs):\n",
    "        h1 = self.layer1(inputs)\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h2 = self.layer2(h1)\n",
    "        h2 = tf.nn.relu(h2)\n",
    "        return self.layer3(h2)\n",
    "    \n",
    "my_block = MyBlock()\n",
    "print('trainable weights:', len(my_block.trainable_weights))\n",
    "y = my_block(tf.ones(shape=(3, 64)))\n",
    "# 构建网络在build()里面，所以执行了才有网络\n",
    "print('trainable weights:', len(my_block.trainable_weights)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 可以通过构建网络层的方法来收集loss\n",
    "class LossLayer(layers.Layer):\n",
    "  \n",
    "  def __init__(self, rate=1e-2):\n",
    "    super(LossLayer, self).__init__()\n",
    "    self.rate = rate\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "    return inputs\n",
    "\n",
    "class OutLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OutLayer, self).__init__()\n",
    "        self.loss_fun=LossLayer(1e-2)##调用之前定义好的网络层\n",
    "    def call(self, inputs):\n",
    "        return self.loss_fun(inputs)\n",
    "    \n",
    "my_layer = OutLayer()\n",
    "print(len(my_layer.losses)) # 还未call\n",
    "y = my_layer(tf.zeros(1,1))\n",
    "print(len(my_layer.losses)) # 执行call之后\n",
    "y = my_layer(tf.zeros(1,1))\n",
    "print(len(my_layer.losses)) # call之前会重新置0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=278, shape=(), dtype=float32, numpy=0.002479173>]\n",
      "[<tf.Variable 'outer_layer/dense/kernel:0' shape=(1, 32) dtype=float32, numpy=\n",
      "array([[ 0.36982614,  0.15829384,  0.41114604, -0.1738567 , -0.36384574,\n",
      "        -0.41569725, -0.3653594 , -0.16020256, -0.3764801 , -0.30773857,\n",
      "        -0.1286354 ,  0.00701469, -0.3177905 ,  0.38692498, -0.0770219 ,\n",
      "         0.33967847, -0.2363088 , -0.2834754 , -0.37630484, -0.02641556,\n",
      "         0.28634483,  0.28438884,  0.08040458,  0.21643132,  0.40347278,\n",
      "        -0.02424791, -0.02258158, -0.0307191 , -0.33047134, -0.1508998 ,\n",
      "         0.32122374, -0.36606735]], dtype=float32)>, <tf.Variable 'outer_layer/dense/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# ##如果中间调用了keras网络层，里面的正则化loss也会被加入进来\n",
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "my_layer = OuterLayer()\n",
    "y = my_layer(tf.zeros((1,1)))\n",
    "print(my_layer.losses) \n",
    "print(my_layer.weights) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "trainable weights: 4\n",
      "[<tf.Tensor: id=1211, shape=(), dtype=float32, numpy=0.041968748>]\n",
      "[<tf.Variable 'my_block_dense_norm_relu_drop_relu/dense_15/kernel:0' shape=(64, 32) dtype=float32, numpy=\n",
      "array([[ 0.05467254,  0.16974711,  0.00301045, ..., -0.17138743,\n",
      "         0.22536135,  0.19827598],\n",
      "       [ 0.13767499,  0.11382854,  0.00651169, ...,  0.18844402,\n",
      "        -0.04664928,  0.18892705],\n",
      "       [ 0.2296558 ,  0.07903302, -0.13097149, ...,  0.09964603,\n",
      "        -0.00492388, -0.04274988],\n",
      "       ...,\n",
      "       [-0.07860452,  0.19541878, -0.06114101, ..., -0.10381567,\n",
      "         0.09945786,  0.1345439 ],\n",
      "       [-0.23823732, -0.09354186,  0.14720261, ..., -0.07961458,\n",
      "         0.17681861,  0.23255426],\n",
      "       [ 0.01601791,  0.21351051, -0.06572437, ..., -0.16238868,\n",
      "        -0.20642775, -0.20408392]], dtype=float32)>, <tf.Variable 'my_block_dense_norm_relu_drop_relu/dense_15/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'my_block_dense_norm_relu_drop_relu/batch_normalization_8/gamma:0' shape=(32,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>, <tf.Variable 'my_block_dense_norm_relu_drop_relu/batch_normalization_8/beta:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'my_block_dense_norm_relu_drop_relu/batch_normalization_8/moving_mean:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'my_block_dense_norm_relu_drop_relu/batch_normalization_8/moving_variance:0' shape=(32,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#全连接层（无激活函数）->batchnorm->sigma(激活函数)->dropout()\n",
    "# 如果中间调用了keras网络层，里面的正则化loss也会被加入进来\n",
    "\n",
    "\n",
    "\n",
    "#2.使用子层递归构建网络层\n",
    "class MyBlock_dense_norm_relu_drop_relu(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyBlock_dense_norm_relu_drop_relu, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "        self.batch_norm=layers.BatchNormalization()\n",
    "        self.drop_out=layers.Dropout(0.5)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h1 = self.dense(inputs)\n",
    "        h1 = self.batch_norm(h1) \n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h1=self.drop_out(h1)\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        return h1\n",
    "    \n",
    "my_block = MyBlock_dense_norm_relu_drop_relu()\n",
    "print('trainable weights:', len(my_block.trainable_weights))\n",
    "y = my_block(tf.ones(shape=(3, 64)))\n",
    "# 构建网络在build()里面，所以执行了才有网络\n",
    "print('trainable weights:', len(my_block.trainable_weights)) \n",
    "print(my_block.losses) \n",
    "print(my_block.weights) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class dense_norm_Layer_relu(layers.Layer):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(dense_norm_Layer_relu, self).__init__()\n",
    "#         self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "# #         self.batch_norm=layers.BatchNormalization(self.dense)\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "# #          h1 = self.batch_norm(inputs)\n",
    "#          h1=self.dense(inputs)\n",
    "\n",
    "# #          h1=layers.BatchNormalization(h1)\n",
    "#          h1 = tf.nn.relu(h1)\n",
    "#          return h1\n",
    "\n",
    "\n",
    "# my_layer = dense_norm_Layer_relu()\n",
    "# y = my_layer(tf.zeros((1,1)))\n",
    "# print(my_layer.losses) \n",
    "# print(my_layer.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.其他网络层配置\n",
    "# 使自己的网络层可以序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainable': True, 'name': 'linear', 'dtype': None, 'units': 125}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units':self.units})\n",
    "        return config\n",
    "    \n",
    "    \n",
    "layer = Linear(125)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置只有训练时可以执行的网络层\n",
    "class MyDropout(layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(MyDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        ###类似if else,此处意思是if training,excute tf.nn.dropout,else inputs\n",
    "        return tf.cond(training, \n",
    "                       lambda: tf.nn.dropout(inputs, rate=self.rate),\n",
    "                      lambda: inputs)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.构建自己的模型\n",
    "######## 通常，我们使用Layer类来定义内部计算块，并使用Model类来定义外部模型 - 即要训练的对象。\n",
    "\n",
    "# Model类与Layer的区别：\n",
    "\n",
    "# 它公开了内置的训练，评估和预测循环（model.fit(),model.evaluate(),model.predict()）。\n",
    "# 它通过model.layers属性公开其内层列表。\n",
    "# 它公开了保存和序列化API。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面通过构建一个变分自编码器（VAE），来介绍如何构建自己的网络。\n",
    "# 采样网络\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5*z_log_var) * epsilon\n",
    "# 编码器\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim=32, \n",
    "                intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h1 = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(h1)\n",
    "        z_log_var = self.dense_log_var(h1)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "        \n",
    "# 解码器\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, \n",
    "                 intermediate_dim=64, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "    def call(self, inputs):\n",
    "        h1 = self.dense_proj(inputs)\n",
    "        return self.dense_output(h1)\n",
    "    \n",
    "# 变分自编码器\n",
    "class VAE(tf.keras.Model):\n",
    "    \n",
    "    ##init的时候定义层\n",
    "    def __init__(self, original_dim, latent_dim=32, \n",
    "                intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super(VAE, self).__init__(name=name, **kwargs)\n",
    "    \n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                              intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim=original_dim,\n",
    "                              intermediate_dim=intermediate_dim)\n",
    "        \n",
    "    ##call的时候定义前向传播\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        kl_loss = -0.5*tf.reduce_sum(\n",
    "            z_log_var-tf.square(z_mean)-tf.exp(z_log_var)+1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.8517\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0691\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6176acedd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "vae = VAE(784,32,64)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "##使用自带的方法进行训练，无法控制每一个batch\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(268.53214, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(9.374093, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(4.7623854, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(3.2093432, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(2.4282207, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(1.9589478, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(1.645642, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(1.4212394, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(1.2527492, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(1.1216668, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(1.0791163, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 自己编写训练方法\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "original_dim = 784\n",
    "vae = VAE(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, x_batch_train in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      reconstructed = vae(x_batch_train)\n",
    "      # Compute reconstruction loss\n",
    "      loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "      loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "      \n",
    "    ##gradiedttype可以直接进行微分处理\n",
    "    \n",
    "    ##计算loss关于，vae.trainable_variables的梯度\n",
    "    grads = tape.gradient(loss, vae.trainable_variables)\n",
    "    ##运行优化器，梯度下降仅仅是其中一个步骤\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "    \n",
    "    loss_metric(loss)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "      print('step %s: mean loss = %s' % (step, loss_metric.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization/gamma:0', 'resnet_identity_block/batch_normalization/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_1/gamma:0', 'resnet_identity_block/batch_normalization_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_2/gamma:0', 'resnet_identity_block/batch_normalization_2/beta:0']\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)###训练时设置为training\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
