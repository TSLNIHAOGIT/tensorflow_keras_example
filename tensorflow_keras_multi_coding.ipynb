{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###tensorflow keras\n",
    "# 自定义模型\n",
    "# 自定义层\n",
    "# 添加early_stopping\n",
    "# 控制每一个batch的输入\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TensorFlow 2.0 中，我最喜欢的一点就是它提供了多个抽象化（abstraction）级别，让你可以根据自己的项目，挑选出最适合的级别。本文中，我将解读如何权衡创建神经网络的两种样式：\n",
    "\n",
    "# # 第一种是符号式（symbolic），符号式 API也称作声明式（Declarative） API。，即你通过操作层次图来创建模型；\n",
    "# # 第二种是命令式（imperative），即你通过扩展类来创建模型。\n",
    "\n",
    "# # 一.符号式（symbolic）：也称作声明式（Declarative） API\n",
    "#    （1）Sequential API，是针对堆栈图的 API\n",
    "#         model = tf.keras.Sequential()\n",
    "        \n",
    "#         model.add(layers.Dense(32, activation='relu'))\n",
    "#         model.add(layers.Dense(32, activation='relu'))\n",
    "#         model.add(layers.Dense(10, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#                      loss=tf.keras.losses.categorical_crossentropy,\n",
    "#                      metrics=['accuracy'])\n",
    "#         model.fit(train_x, train_y, batch_size=32, epochs=5)\n",
    "\n",
    "#    （2）Functional API，是针对 DAG 的 API；\n",
    "#         用 Functional API 创建多输入/多输出模型，可以操作非线性拓扑、共享层的模型以及有多个输入或输出的模型\n",
    "#         input_x = tf.keras.Input(shape=(72,))\n",
    "#         hidden1 = layers.Dense(32, activation='relu')(input_x)\n",
    "#         hidden2 = layers.Dense(16, activation='relu')(hidden1)\n",
    "#         pred = layers.Dense(10, activation='softmax')(hidden2)\n",
    "        \n",
    "#         model = tf.keras.Model(inputs=input_x, outputs=pred)\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#                      loss=tf.keras.losses.categorical_crossentropy,\n",
    "#                      metrics=['accuracy'])\n",
    "#         model.fit(train_x, train_y, batch_size=32, epochs=5)\n",
    "#    （3）其它符号式API\n",
    "#         TensorFlow v1 (及 Theano) 提供了一个层级更低得多的 API\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     符号式 API 的优势和局限性\n",
    "# 优势\n",
    "\n",
    "#     使用符号式 API 创建的模型，就是一个类似图形的数据架构，这就意味着你的模型可以接受监测或者进行汇总。\n",
    "\n",
    "#     你可以将模型当成图像来为其绘制图表（使用 keras.utils.plot_model）；或者简单地使用 model.summary() 来呈现层、权重以及形状的描述。\n",
    "\n",
    "#     **同样地，在将层拼接在一起时，开发库的设计者可以运行扩展的层兼容性检查（在创建模型时和执行模型之前）。\n",
    "\n",
    "#     **这类似于在编译器中进行类型检查，可以极大地减少开发者的错误。\n",
    "\n",
    "#     大多数的故障排除都会在模型自定义阶段而不是执行期间进行。你可以保障所有编译的模型都能正常运行，这也加速了迭代，并让故障排除变得更简单。\n",
    "\n",
    "#     符号式模型提供了一个一致的 API，这就使得这些模型的重复使用和共享变得简单。例如，在迁移学习中，你可以访问中间层的神经元，从而从现有的神经元中创建新的模型，就像这样：\n",
    "\n",
    "#     符号式模型由可自然地进行复制和克隆的数据架构进行定义。\n",
    "#       **例如，Sequential API 和 Functional API 可以提供 model.get_config()，model.to_json()，model.save()，clone_model(model)，同时仅凭借数据架构就能够重新创建同样地模型（而不需要访问用来定义和训练模型的原始代码）。\n",
    "\n",
    "\n",
    "        \n",
    "#      虽然精心设计的 API 应该跟神经网络的心智模型匹配，但是跟我们作为一个程序员所有的心智模型进行匹配也同样重要。对于我们大多数程序员来说，这种心智模型就是命令式的编程样式。在符号式 API 中，你操作「声明式的张量」（这些张量是没有值的）来创建图表。Keras 的 Sequential API 和 Functional API「感觉像」命令性的，它们是在开发者没有意识到他们在用符号定义模型的情况下被设计出来的。\n",
    "\n",
    "# 局限性\n",
    "# **符号式 API 的当前一代，可以很好地适用于有**向无环图的模型创建，这可以满足绝大多数实际应用的需要，\n",
    "# 然而现在也有一些特例无法匹配这个简洁的抽象化，**例如，树形循环神经网络和递归神经网络等动态网络。\n",
    "\n",
    "# 这也是为什么 TensorFlow 要同时还提供命令式的模型创建 API 样式（上文中提到的子类化 API）。无论是使用 Sequential API 还是 Functional API，\n",
    "# 你都会用到所有熟悉的层、初始化器以及优化器。\n",
    "# **同时，这两类 API 是完全互操作的，\n",
    "# 因此你可以混合并且搭配两者使用**（例如将一种模型嵌套到另一种模型中）。\n",
    "# 你可以采用一个符号式模型并在**子类化模型中将它用作层，反之亦然。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #二.命令式（Imperative）API  :也称作模型子类化（Model Subclassing） API\n",
    "#    在命令式 API 中，你要像编写 NumPy 一样编写模型。用这种 API 创建模型感觉像是在开发面向对象的 Python\n",
    "#     从一个开发者的角度，它工作的方法就是扩展由框架定义的模型类别，将模型中的层实例化，然后命令式地编写下模型的正向传递（forward pass），而反向传递（backward pass）是自动生成的。\n",
    "#     class MyModel(tf.keras.Model):\n",
    "#         #在 init 方法中创建层并将它们设置为类实例的属性\n",
    "#         def __init__(self, num_classes=10):\n",
    "#             super(MyModel, self).__init__(name='my_model')\n",
    "#             self.num_classes = num_classes\n",
    "#             self.layer1 = layers.Dense(32, activation='relu')\n",
    "#             self.layer2 = layers.Dense(num_classes, activation='softmax')\n",
    "#         #在 call 方法中定义前向传播\n",
    "#         def call(self, inputs):\n",
    "#             h1 = self.layer1(inputs)\n",
    "#             out = self.layer2(h1)\n",
    "#             return out\n",
    "\n",
    "#         def compute_output_shape(self, input_shape):\n",
    "#             shape = tf.TensorShapej(input_shape).as_list()\n",
    "#             shape[-1] = self.num_classes\n",
    "#             return tf.TensorShape(shape)\n",
    "\n",
    "#     model = MyModel(num_classes=10)\n",
    "#     model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "#                  loss=tf.keras.losses.categorical_crossentropy,\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "#     model.fit(train_x, train_y, batch_size=16, epochs=5)\n",
    "    \n",
    "    \n",
    "# 命令式 API 的优势和局限性\n",
    "# 优势\n",
    "\n",
    "# **正向传递（forward pass）以命令式的方法编写，这就使得用自己的实现来替换掉通过开发库实现的部分（例如一层、一个神经元后者一个损失函数）变得很容易。这种方式的编程也非常自然，并且是深入了解深度学习的基本要点的不错的方法。\n",
    "\n",
    "# **这也让你快速地尝试新想法变得很容易（深度学习开发工作流会变得与面向对象的 Python 一样），同时对于研究人员来说尤其有帮助。\n",
    "\n",
    "# **也可以很轻易地使用 Python 指定模型正向传递中的任意控制流。\n",
    "#  ———————————————— \n",
    "    \n",
    "    \n",
    "# 局限性\n",
    "\n",
    "# **当使用命令式 API 使，模型是由某个类别方法来进行定义的。这样的话，模型就不再是一个清晰的数据架构，而是一个不透明的字节码。这种 API 样式所获得的灵活性是以可用性和可重用性换来的。\n",
    "\n",
    "# **故障排除发生在执行期间，而不是在定义模型之时。\n",
    "#   使用这一 API 样式时，由于几乎不会对输入或者层间兼容性进行检查，因此大量的故障排除压力就从框架上转移到了开发者身上。\n",
    "   \n",
    "# 命令式模型很难进行重复利用。例如你无法使用一个一致的 API 去访问中间层或神经元\n",
    "\n",
    "    \n",
    "# 相反地，提取神经元的方法就是采用一种新的调用（或者前进）方法来编写一个新的类别。最开始的时候可能会觉得这个操作有趣又简单，但是如果没有标准的话就会积累成技术债（tech debt）。 \n",
    "\n",
    "# 命令式模型也很难进行检测、复制和克隆。\n",
    "\n",
    "# **例如，model.save()， model.get_config()，以及 clone_model 对于子类化的模型是不起作用的，\n",
    "# 而 model.summary() 也只能给你层的列表（并且不会提供任何关于它们怎样进行连接的信息，因为这些信息是访问不了的）\n",
    "\n",
    "\n",
    "\n",
    "# 总结\n",
    "# TensorFlow 2.0 直接支持符号式 API 和命令式 API 两种样式，因此大家可以选择最适合自己项目的抽象化（复杂性）层级。\n",
    "\n",
    "# 如果你的目标是易用、低预算，同时你倾向于将模型考虑称层次图，那就使用 Keras 的 Sequential API 或者 Functional API (就像拼装乐高积木一样) 和内建的训练循环。这种方法适用于大多数问题。\n",
    "\n",
    "# 如果你偏好于将模型考虑成面向对象的 Python/Numpy 开发者，同时有限考虑模型的灵活性和可破解性，Keras 的 Subclassing 这样的 API 会比较适合你。\n",
    "\n",
    "    \n",
    "    \n",
    "# 训练循环（Training Lloop）\n",
    "#     自定义的模型无论是使用 Sequential API、Functional API 还是使用子类化样式，都可以用两种方式进行训练：\n",
    "\n",
    "# (1)一种是使用内建的训练路径和损失函数（第一个示例讲到的，我们使用的是 model.fit 和 model.compile）；\n",
    "\n",
    "# (2)另一种是定制更复杂的训练循环（例如，当你想要自行编写梯度裁剪代码时）或损失函数，你可以按照以下方法轻易实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=500)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最常见的模型类型是层的堆叠：tf.keras.Sequential 模型\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# ##可以model.add()添加层，也可以直接将列表加进去\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "# # Adds a densely-connected layer with 64 units to the model:\n",
    "# layers.Dense(64, activation='relu'),\n",
    "# # Add another:\n",
    "# layers.Dense(64, activation='relu'),\n",
    "# # Add a softmax layer with 10 output units:\n",
    "# layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              ##上面的优化函数才是对的，tf.train.AdamOptimizer不能用\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.data输入数据并进行训练\n",
    "import numpy as np\n",
    "\n",
    "train_x = np.random.random((1000, 72))\n",
    "train_y = np.random.random((1000, 10))\n",
    "\n",
    "val_x = np.random.random((200, 72))\n",
    "val_y = np.random.random((200, 10))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "val_dataset = val_dataset.repeat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=10, steps_per_epoch=30,\n",
    "          validation_data=val_dataset, validation_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 4ms/step - loss: 11.4649 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.464878495534261, 0.1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.4评估与预测\n",
    "test_x = np.random.random((1000, 72))\n",
    "test_y = np.random.random((1000, 10))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "test_data = test_data.batch(32).repeat()\n",
    "model.evaluate(test_data, steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09627154 0.09594291 0.10508692 ... 0.10022056 0.10034806 0.09685513]\n",
      " [0.10083509 0.10153824 0.10806082 ... 0.0947385  0.09095731 0.09853249]\n",
      " [0.10834585 0.09854479 0.10336304 ... 0.10329025 0.09235978 0.09772509]\n",
      " ...\n",
      " [0.09684049 0.10333864 0.09633884 ... 0.09816224 0.10727387 0.10667883]\n",
      " [0.10437717 0.10256477 0.09875691 ... 0.09714945 0.1054528  0.0862847 ]\n",
      " [0.10715229 0.09713148 0.10328652 ... 0.09553403 0.08844198 0.0949506 ]]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "result = model.predict(test_x, batch_size=32)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# 4.构建高级模型\n",
    "# 4.1函数式api\n",
    "# tf.keras.Sequential 模型是层的简单堆叠，无法表示任意模型。使用 Keras 函数式 API 可以构建复杂的模型拓扑，例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 437us/sample - loss: 11.7316 - accuracy: 0.0930\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 140us/sample - loss: 11.5862 - accuracy: 0.0960\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 122us/sample - loss: 11.5750 - accuracy: 0.1140\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 11.5670 - accuracy: 0.0920\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 11.5627 - accuracy: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ad04dad30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = tf.keras.Input(shape=(72,))\n",
    "hidden1 = layers.Dense(32, activation='relu')(input_x)\n",
    "hidden2 = layers.Dense(16, activation='relu')(hidden1)\n",
    "pred = layers.Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_x, outputs=pred)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "             loss=tf.keras.losses.categorical_crossentropy,\n",
    "             metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# '''\n",
    "# 4.2模型子类化：自定义模型通过模型子类化实现\n",
    "# 通过对 tf.keras.Model 进行子类化并定义您自己的前向传播来构建完全可自定义的模型。\n",
    "# 在 init 方法中创建层并将它们设置为类实例的属性。在 call 方法中定义前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 559us/sample - loss: 11.7106 - accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 264us/sample - loss: 11.6248 - accuracy: 0.1140\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 244us/sample - loss: 11.5983 - accuracy: 0.1140\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 266us/sample - loss: 11.5767 - accuracy: 0.1290\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 11.5640 - accuracy: 0.1340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ad0267080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    #在 init 方法中创建层并将它们设置为类实例的属性\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = layers.Dense(32, activation='relu')\n",
    "        self.layer2 = layers.Dense(num_classes, activation='softmax')\n",
    "    #在 call 方法中定义前向传播,将之前的层按照前向传播连起来\n",
    "    def call(self, inputs):\n",
    "        h1 = self.layer1(inputs)\n",
    "        out = self.layer2(h1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    ###计算输出层的维度\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShapej(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)\n",
    "\n",
    "model = MyModel(num_classes=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "             loss=tf.keras.losses.categorical_crossentropy,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##自定义层通过层的子类化实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 01:20:53.330900 140521470113536 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 11.5042 - accuracy: 0.1020\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 309us/sample - loss: 11.4948 - accuracy: 0.1070\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 226us/sample - loss: 11.4892 - accuracy: 0.1120\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 11.4837 - accuracy: 0.1240\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 11.4804 - accuracy: 0.1260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd357c3da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4.3自定义层\n",
    "通过对 tf.keras.layers.Layer 进行子类化并实现以下方法来创建自定义层：\n",
    "使用 add_weight 方法添加权重。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "\n",
    "    #build：创建层的权重。\n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "        self.kernel = self.add_weight(name='kernel1', shape=shape,\n",
    "                                   initializer='uniform', trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)\n",
    "    #call：定义前向传播。\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "    #compute_output_shape：指定在给定输入形状的情况下如何计算层的输出形状。\n",
    "    #或者，可以通过实现 get_config 方法和 from_config 类方法序列化层。\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.output_dim\n",
    "        return tf.TensorShape(shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "model = tf.keras.Sequential(\n",
    "[\n",
    "    MyLayer(10),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "             loss=tf.keras.losses.categorical_crossentropy,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=5)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
